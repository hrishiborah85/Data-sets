{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/kaggle/input/rossmann-store-sales/train.csv')\n",
    "store=pd.read_csv('/kaggle/input/rossmann-store-sales/store.csv')\n",
    "test=pd.read_csv('/kaggle/input/rossmann-store-sales/test.csv')\n",
    "print(data.shape)\n",
    "print(store.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()[['Sales','Customers']].loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()[['Sales','Customers']].loc['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()[['Sales','Customers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Store.nunique()\n",
    "data.Store.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Store.value_counts().head(20).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.DayOfWeek.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Open.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Store.nunique()\n",
    "data.Promo.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date']=pd.to_datetime(data['Date'],format='%Y-%m-%d')\n",
    "store_id=data.Store.unique()[0]                 #unique()[num] num is store_id\n",
    "print(store_id)\n",
    "store_rows=data[data['Store']==store_id]\n",
    "print(store_rows.shape)\n",
    "store_rows.resample('1d',on='Date')['Sales'].sum().plot.line(figsize=(14,4))           #helps to identify missing dates and hence accordingly line plot will be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rows=data[data['Store']==store_id]\n",
    "print(store_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rows[store_rows['Sales']==0]              #open=0 so when store is closed sales =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Date']=pd.to_datetime(test['Date'],format='%Y-%m-%d')\n",
    "store_test_rows=test[test['Store']==store_id]                              #same store_id taken fr test data\n",
    "store_test_rows['Date'].min(),store_test_rows['Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_test_rows['Open'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rows['Sales'].plot.hist()          #here not perfectly skew and need to identify whether 0 values are needed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 should we retain row with zero sales?\n",
    "# 2 since sales column is right skewed transformation?\n",
    "# 3 missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store[store['Store']==store_id].T #here we cannont fill promo2 since year missing values with zero as it has year dd-mm-yy in it and 0 makes no sense\n",
    "#for promo2since week we can feel missing values with zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store[~store['Promo2SinceYear'].isna()]\n",
    "#taking rows with notnull values                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking first row\n",
    "store[~store['Promo2SinceYear'].isna()].iloc[0]\n",
    "#taking rows with notnull values                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store['Promo2SinceWeek']=store['Promo2SinceWeek'].fillna(0)\n",
    "store['Promo2SinceYear']=store['Promo2SinceYear'].fillna(store['Promo2SinceYear'].mode().iloc[0])\n",
    "store['PromoInterval']=store['PromoInterval'].fillna(store['PromoInterval'].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store['CompetitionDistance'] = store['CompetitionDistance'].fillna(store['CompetitionDistance'].max())                               \n",
    "store['CompetitionOpenSinceMonth']=store['CompetitionOpenSinceMonth'].fillna(store['CompetitionOpenSinceMonth'].mode().iloc[0])               \n",
    "store['CompetitionOpenSinceYear']=store['CompetitionOpenSinceYear'].fillna(store['CompetitionOpenSinceYear'].mode().iloc[0])\n",
    "store.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if sequential then go for backward and forward fill method-- eg stock market analysis mon-friday sat sun no values can ill it with fridays data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged=data.merge(store,on='Store',how='left')\n",
    "print(data.shape)\n",
    "print(data_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding\n",
    "# 3 categorical column,1 datecolumn,rest are numerical\n",
    "#data_merged.dtyes\n",
    "data_merged['day']=data_merged['Date'].dt.day\n",
    "data_merged['month']=data_merged['Date'].dt.month\n",
    "data_merged['year']=data_merged['Date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for linear regresson and logistic regression use 1 hoc encoding\n",
    "#for decisio tree we can use label encodig\n",
    "data_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding on stateholiday,store type,assortment,promo interval       #label encoding done as we use decsion tree algorithm for linear regression\n",
    "data_merged['StateHoliday'].unique()\n",
    "data_merged['StateHoliday']=data_merged['StateHoliday']=data_merged['StateHoliday'].map({'0':0,0:0,'a':1,'b':2,'c':3})   #coded '0',0,a,b,c \n",
    "data_merged['StateHoliday']=data_merged['StateHoliday'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while using decision trees standardization is not needed\n",
    "#if target column required in decision tree may need to scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged['Assortment']=data_merged['Assortment'].map({'a':1,'b':2,'c':3})   #coded a,b,c\n",
    "data_merged['Assortment']=data_merged['Assortment'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged['StoreType']=data_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\n",
    "data_merged['StoreType']=data_merged['StoreType'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_promo={'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}\n",
    "data_merged['PromoInterval']=data_merged['PromoInterval'].map(map_promo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data_merged.columns.drop(['Sales','Date','Customers'])   #customers added later on for dropping not now discussed below by dropped\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x,validate_x,train_y,validate_y=train_test_split(data_merged[features],np.log(data_merged['Sales']+1),test_size=0.2)           #if target variable has lot of zero vvalues use log(x+1) transform\n",
    "#here test_size=0.2 is for validate_x wichis like test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape,validate_x.shape,train_y.shape,validate_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply decision tree algorithm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_dt=DecisionTreeRegressor(max_depth=11,random_state=1).fit(train_x,train_y)\n",
    "validate_y_pred=model_dt.predict(validate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_tree(model, columns):\n",
    "#     import pydotplus\n",
    "#     from sklearn.externals.six import StringIO\n",
    "#     from IPython.display import Image\n",
    "#     import os\n",
    "#     from sklearn import tree\n",
    "    \n",
    "#     graphviz_path = 'C:\\Program Files (x86)\\Graphviz2.38/bin/'\n",
    "#     os.environ[\"PATH\"] += os.pathsep + graphviz_path\n",
    "\n",
    "#     dot_data = StringIO()\n",
    "#     tree.export_graphviz(model,\n",
    "#                          out_file=dot_data,\n",
    "#                          feature_names=columns)\n",
    "#     graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#     return Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_tree(model_dt,features)         #worked only for depth =3 #plotting decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_y_pred=model_dt.predict(validate_x)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "validate_y_inv=np.exp(validate_y)-1\n",
    "validate_y_pred_inv=np.exp(validate_y_pred)-1\n",
    "np.sqrt(mean_squared_error(validate_y_inv,validate_y_pred_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customer column is not present in test set do we drop it or addi t in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt               #for checking highest important feature\n",
    "plt.figure(figsize=(10,5))                    \n",
    "yvalues=model_dt.feature_importances_\n",
    "xvalues=features\n",
    "plt.bar(xvalues,yvalues)\n",
    "plt.xticks(rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customer column is the most important feauture so cannot drop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one way of checking the important features is using heatmap or the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.corr().loc['Sales'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding customer column in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_avg_custs=data.groupby(['Store'])[['Customers']].mean().reset_index().astype(int)     # taking mean customers in each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1=test.merge(stores_avg_custs,on='Store',how='left')\n",
    "test.shape,test_1.shape\n",
    "test_merged=test_1.merge(store,on='Store',how='left')\n",
    "test_merged.isna().sum()                             #missing values appearing as merged the data with a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['Open']=test_merged['Open'].fillna(1)\n",
    "test_merged['Date']=pd.to_datetime(test_merged['Date'],format='%Y-%m-%d')\n",
    "test_merged['day']=test_merged['Date'].dt.day\n",
    "test_merged['month']=test_merged['Date'].dt.month\n",
    "test_merged['year']=test_merged['Date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the object types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['StateHoliday'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['StateHoliday']=test_merged['StateHoliday'].map({'0':0,'a':1})   #coded a,b,c\n",
    "test_merged['Assortment']=data_merged['Assortment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['Assortment']=data_merged['Assortment'].map({'a':1,'b':2,'c':3})   #coded a,b,c\n",
    "test_merged['Assortment']=data_merged['Assortment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['StoreType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['StoreType']=data_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\n",
    "test_merged['StoreType']=data_merged['StoreType'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_promo={'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}\n",
    "test_merged['PromoInterval']=test_merged['PromoInterval'].map(map_promo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=model_dt.predict(test_merged[features])\n",
    "test_pred_inv=np.exp(test_pred)-1\n",
    "submission_predicted=pd.DataFrame({'ID':test['Id'],'Sales':test_pred_inv})\n",
    "submission_predicted.to_csv('submission.csv',index=False)\n",
    "submission_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ToWeight(y):\n",
    "#     w = np.zeros(y.shape, dtype=float)\n",
    "#     ind = y != 0\n",
    "#     w[ind] = 1./(y[ind]**2)\n",
    "#     return w\n",
    "\n",
    "# def rmspe(y, yhat):\n",
    "#     w = ToWeight(y)\n",
    "#     rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "#     return rmspe\n",
    "# validate_y_inv=np.exp(validate_y)-1\n",
    "# validate_y_pred_inv=np.exp(validate_y_pred)-1\n",
    "# rmse_val=np.sqrt(mean_squared_error(validate_y_inv,validate_y_pred_inv))\n",
    "# rmspe_val=rmspe(validate_y_inv,validate_y_pred_inv)\n",
    "# print(rmse_val,rmspe_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open==0 ? should we keepthese rows\n",
    "# missing value treatment\n",
    "# necoding\n",
    "# hyperparameters which model?\n",
    "# adding customers clumn to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will try by dropping customers column\n",
    "#features=data_merged.columns.drop(['Customers']) will be added above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hyperparameter tuning\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters={'max_depth':list(range(5,20))}   #randomly search within depths of 5-15\n",
    "# base_model=DecisionTreeRegressor()\n",
    "# cv_model=GridSearchCV(base_model,param_grid=parameters,cv=5,return_train_score=True).fit(train_x,train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.best_params_  #do not go for the best params result as it may give wrong result as it tries to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# df_cv_results=pd.DataFrame(cv_model.cv_results_).sort_values(by='mean_test_score',ascending=False)\n",
    "# df_cv_results.set_index('param_max_depth')['mean_test_score'].plot.line()\n",
    "# df_cv_results.set_index('param_max_depth')['mean_train_score'].plot.line()\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the point where the lines starts diverging will be best param max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results[df_cv_results['param_max_depth']==11].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rscore of the test and the train in cv is meanscore(+-)std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for max depth =11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## apply decision tree algorithm\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# model_dt=DecisionTreeRegressor(max_depth=11,random_state=1).fit(train_x,train_y)\n",
    "# validate_y_pred=model_dt.predict(validate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred=model_dt.predict(test_merged[features])\n",
    "# test_pred_inv=np.exp(test_pred)-1\n",
    "# submission_predicted=pd.DataFrame({'ID':test['Id'],'Sales':test_pred_inv})\n",
    "# submission_predicted.to_csv('submission.csv',index=False)\n",
    "# submission_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ToWeight(y):\n",
    "#     w = np.zeros(y.shape, dtype=float)\n",
    "#     ind = y != 0\n",
    "#     w[ind] = 1./(y[ind]**2)\n",
    "#     return w\n",
    "\n",
    "# def rmspe(y, yhat):\n",
    "#     w = ToWeight(y)\n",
    "#     rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "#     return rmspe\n",
    "# validate_y_inv=np.exp(validate_y)-1\n",
    "# validate_y_pred_inv=np.exp(validate_y_pred)-1\n",
    "# rmse_val=np.sqrt(mean_squared_error(validate_y_inv,validate_y_pred_inv))\n",
    "# # rmspe_val=rmspe(validate_y_inv,validate_y_pred_inv)\n",
    "# print(rmse_val,rmspe_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rmspe_score(model,input_values,y_actual):\n",
    "#     y_predicted=model.predict(input_values)\n",
    "#     y_actual=np.exp(y_actual)-1\n",
    "#     y_predicted=np.exp(y_predicted)-1\n",
    "#     score=rmspe(y_actual,y_predicted)\n",
    "#     return score\n",
    "# parameters={'max_depth':list(range(5,20))}  \n",
    "# base_model=DecisionTreeRegressor()\n",
    "# cv_model=GridSearchCV(base_model,param_grid=parameters,cv=5,return_train_score=True,scoring=get_rmspe_score).fit(train_x,train_y)\n",
    "# pd.DataFrame(cv_model.cv_results_)[['params','mean_test_score','mean_train_score']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value treatment\n",
    "#encoding\n",
    "#tranformation\n",
    "#model fitting hyper parameter tuning\n",
    "#prediction and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain=xgb.DMatrix(train_x[features],train_y)\n",
    "dvalidate=xgb.DMatrix(validate_x[features],validate_y)\n",
    "params={'max_depth':11,'eta':0.1,'objective':'reg:linear'}\n",
    "model_xg=xgb.train(params,dtrain,200)\n",
    "predict_y=model_xg.predict(dvalidate)\n",
    "\n",
    "validate_y_inv=np.exp(validate_y)-1\n",
    "validate_y_pred_inv=np.exp(validate_y_pred)-1\n",
    "rmspe_val=rmspe(validate_y_inv,validate_y_pred_inv)\n",
    "print(rmspe_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
